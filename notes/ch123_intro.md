카프카의 탄생

Linkedin에서 파편화된 데이터 수집 및 분배 아키텍처를 운영하는 데에 어려움을 겪으면서 소스와 타깃 어플리케이션을 연결하는 프레임워크를 만든것이다.

기존에는 각종 상용 데이터 프레잌워크와 오픈소스를 아키텍처에 녹여서 데이터 파이프라인의 파편화를 개선하려 했으나 파이프라인의 복잡도를 개선하지 못했다.

그래서 링크드인의 데이터팀은 신규 시스템을 만들기로 결정했고 그 결과가 카프카다. 카프카는 각각의 어플리케이션끼리 연결하여 한 곳에 모아 처리하도록 했다.


내부구조
토픽: RDBMS의 테이블와 같은 개념. 구분하고자하는 데이터를 나누는 것. 토픽 내에는 파티션이 존재한다. 

데이터는 producer가 보낸다. Producer가 데이터를 한번 보내면, 여러 파티션 중 하나에 적재된다. 파티션 내부 구조는 queue구조와 동일하며 따라서 FIFO를 따른다.

최종적으로는 consumer가 파티션들로부터 데이터를 가져가며, 데이터는 가져가더라도 삭제되지 않는다. 

커밋을 통해 몇번까지 읽었는지 기록한다. 


----
카프카가 데이터 파이프라인으로 적합한 4가지 이유
1. 높은 처리량: 카프카 브로커에 데이터를 전송할 때 별개로 보내는게 아니라, batch로 묶어서 보내기 때문에 최소한의 네트워크 비용으로 최대의 효율을 만들 수 있다. 파티션 단위를 통해 여러 파티션에 분배하고 데이터를 병렬 처리할 수 있다. 파티션을 늘리고, consumer 또한 늘려서 데이터 처리량을 늘릴 수 있다.
2. 확장성: 데이터가 얼마나 들어올지는 예측하기 어렵다. 카프카는 이런 가변적인 환경에서 안정적으로 확장 가능하도록 설계되어있다. 데이터가 많아지면 브로커의 개수를 자연스럽게 늘려 scale-out할 수 있다. 처리량을 늘려서 대응하는것이다.
3. 영속성: 데이터를 생성한 프로그램이 종료되더라도 사라지지 않는 데이터의 특성을 뜻한다. 다른 메시징 플랫폼과 다르게 전송받은 데이터를 메모리에 저장하지 않고 파일 시스템에 저장한다. 페이지 캐시 메모리 영역을 사용하여 메모리에 저장시켰다가 다시 사용하는 방식이라 카프카 파일 시스템에 저장하고 데이터를 저장, 전송하더라도 처리량이 높다. producer가 데이터를 하나 보내더라도 broker라는 processor에 장애가 생겨도 데이터를 복구시켜서 disk에 저장되어 있는 내용을 살릴 수 있다.
4. 고가용성: 3개 이상의 서버들로 운영되는 카프카 클러스터는 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다. kafka cluster 안에 broker라는 process를 3개 이상 실행 시키는데, producer가 데이터를 보낼 때 한개의 broker에 저장되는게 아니라, 복제가 되어서 2 3에도 저장된다. producer으로부터 하나의 broker에 장애가 생겨도 나머지 broker에 데이터가 적재되어 있어서 처리가 가능한 성질을 고가용성이라 한다. 즉, 끊임없이 데이터를 처리할 수 있게 된다. 

------
빅데이터 아키텍처의 종류와 카프카의 미래
최초의 데이터를 원천 데이터 (raw data)
파생 데이터: 어떤 가공을 거친 데이터가 파생 데이터가 되며,
이를 데이터 과학자들이 사용할 수 있게 해준게 서빙 데이터다.

초기 빅데이터 플랫폼은 end-to-end로, 즉 시작부터 끝까지 데이터를 배치로 모았으나, 이건 유연하지 못하며 data governance (데이터 표준 및 정책)을 지키기 어려웠다. 

람다 아키텍처: 그래서 생긴게 람다 아키텍처다. 이는 3가지 레이어로 나뉘며 이들은 각각 1) batch layer, 2) serving layer 3) speed layer이다. 

1. 특정 시간, 타이밍마다 일괄 처리되는 레이어. 
2. 가공된 데이터를 사용할 수 있도록 데이터가 저장된 공간.
3. 서비스에서 생성되는 데이터를 실시간으로 분석하는 용도.

Kafka는 speed layer에 해당한다. 

람다 아키텍처의 한계: 배치 데이터와 실시간 데이터를 융합해서 처리 해야 할때 유연하지 않은 파이프라인을 생성해야 한다는 것. 데이터를 2개의 레이어 (배치, 스피드)로 나뉘면서 분석할 때 또한 2벌로 각각의 레이어에 대해 로직이 존재해야 한다는 것. 디버깅도 따로따로. batch vs stream

카파 아키텍처
람다 아키텍처의 단점을 해소하기 위해 카파 아키텍처가 생긴다. 배치 레이어를 아예 제거했다. speed layer에서 batch data와 stream data 모두 모아야 한다. 이걸 하기 위해서는 로그에서 timestamp가 있는 레코드를 모으는것이다.

batch data vs stream data
batch: bounded data 처리, 복잡한 키 조인 수행.대규모 배치 데이터를 위한 분산 처리.
stream: unbounded data 처리, 단순한 키 조인 수행, 지속적으로 들어오는 데이터를 위한 분산 처리 수행


batch는 hadoop으로 map reduce로 hdfs에 데이터를 저장하는 형식.

kafka로 batch 데이터를 사용하는 것은 materialized view로 사용한다. 즉 timestamp로 log를 남기는것이다. 

streaming data lake architecture
Speed layer만 존재하는 아키텍처이다. 이를 이루기는 쉽지 않지만, '자주 접근하는 데이터' 기준으로 이를 실현하는 방식이 생기고 있다.




-------
Kafka 생태계
프로듀서-> 카프카 클러스터->컨슈머

이외에 각종 툴: 카프카 스트림즈, 커넥트
커넥트 (소스): 프로듀서의 역할을 해주며 mySQL 또는 aws S3에서 데이터를 갖고 온다. 반복적으로 만들수 있는게 이득이다.
커넥트 (싱크): 컨슈머의 역할을 해준다. JDBC 또는 ElasticSearch로 보낸다.

-------
카프카 브로커와 클러스터
브로커.클러스터.주키퍼
2점대에서는 주키퍼가 무조건 필요하고 3점대에서는 필요는 없지만, 아직까지는 필요하다고 생각하면 된다. 

하나의 클러스터에는 여러개의 브로커가 존재할 수 있다. 일반적으로 3개가 있고, 많게는 50개를 사용할 때도 있다.

브로커는 장애가 발생하더라도 안전하게 사용할 수 있도록 도와주는 어플리케이션. 

---------
여러개의 카프카 클러스터가 연결된 주키퍼
주키퍼 앙상블을 통해 여러 클러스터를 동시에 운영할 수 있다. 

---------
브로커 역할, 컨트롤러, 데이터 삭제
컨트롤러는 브로커 한대가 컨트롤러의 역할을 한다. 컨트롤러는 다른 프로커들의 상태를 체크하고 브로커가 클러스터에서 빠지는 경우 해당 브로커에 존재하는 리더 파티션을 재분재한다. 컨트롤러 브로커에 장애가 생기면 다른 브로커가 역할을 위임받는다.

데이터 삭제: 일정 시간 또는 용량에 따라 삭제 기능. 오직 브로커만이 데이터를 삭제할 수 있다. 데이터 삭제는 파일 단위로 이뤄지는데 이를 log segment라고 부른다. 

컨슈머 오프셋 저장: 토픽에 있는 데이터를 가져왔을때 어디까지 처리했는지 알려주는 커밋을 하는데. 커밋한 오프셋은 __consumer_offsets 토픽에 저장한다. 자동으로 생성되며 internal topic이라 불린다.

그룹 코디네이터: 코디네이터는 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할을 한다. 1:1 매핑이 되며, 문제가 되는 컨슈머는 삭제하며 재할당하는 과정 (rebalance) 또한 그룹 코디네이터가 도와준다. 

///////////
레코드-메시지 키
어느 파티션에 들어갈지 정해주는게 파티셔너(partitioner)이다. 메시지 키는 메시지 값를 분류하기 위해 사용되며, 필수는 아니고, 따로 정의되지 않으면 RR을 따른다. 

